
  ___           ___             ___              
 |   \ _____ __/ _ \ _ __ ___  / __|__ _ ___ ___ 
 | |) / -_) V / (_) | '_ (_-< | (__/ _` (_-</ -_)
 |___/\___|\_/ \___/| .__/__/  \___\__,_/__/\___|


TURKCE OKUMAK ISTIYORSAN BENIOKU DOSYASINA GIT

I will describe the steps I took to create a GKE cluster using Terraform.


Step 1 Provider

I created a provider.tf that defines the infrastructure that specifies which provider I will use with Terraform and which service it should interact with.
In this case, I continued by specifying the platform as google and giving descriptive input/output.
After making the necessary arrangements about the Google Storage Bucket to store the Terraform state, I determined the necessary provides versions.

Step 2 VPC

I created VPC with Terraform infrastructure. Here we could also create a Kubernetes Cluster using the existing VPC.
Before we create a new VPC, we need to enable the compute API. To create a GKE Cluster, we can start building the VPC itself after enabling the container in google API.

#resource "google_compute_network" "main" { .... }

After specifying the source name, we choose the routing mode. We can choose one of two options here, Regional or Global.

Since we want to create our own subnet with terraform infrastructure, we set auto_create_subnetworks = false.
We prefer mtu, ( maximum transmission unit) value as a minimum of 1460 in bytes.
When creating the Terraform infrastructure, we select false to determine whether the default routing rules for the network zone should be deleted when creating it.
By choosing this to false, we actually want to keep the default management rules.
We specify dependencies between resources with the depends_on option.

Step 3 Subnet 

We create a special subnet according to the state of Kubernetes nodes.
It is managed by Google from kubernetes control groups in GKE Cluster groups.

It is better to specify the specific name, then we specify the CIDR range of the subnet.
You can create subnets in different regions my preference is here us-central1
We continue the network we created earlier by providing a reference here.
We enable private google ip. Virtual machines in this subnet without external ip addresses can access Google APIs and services.
Then we specify secondary IP ranges
If we need to create a firewall to access other VMs in my Kubernetes VPC network, we can optionally use it on our Kubernetes nodes by specifying the secondary ip range.

Step 4 Routuer

We are creating a Router that will be used with the NAT gateway to allow VMs without public IP addresses to access the internet.

Step 5 NAT

First, we create a resource of type "google_compute_router_nat" and set its name to "nat". This resource contains a router name and specifying the zone (us-central1).
  source_subnetwork_ip_ranges_to_nat = "LIST_OF_SUBNETWORKS"
   nat_ip_allocate_option
Specifies which resources are included in the NAT configuration and how they are retrieved,
In the "subnetwork" block, we specify the specific resources of the subnets specified for NAT.

Step 6 Firewall

We don't need to manually create any firewalls for GKE, but I still configure a firewall with this firewall to allow ssh into the VPC.


Step 7 Kubernetes

A resource of type "google_container_cluster" is created and we set the name to "primary".
The location of this cluster (us-central1-a),
Remove the default node pool, how many nodes will be created initially (we created 1), network and subnet connections (google_compute_network.main.self_link, google_compute_subnetwork.private.self_link), logging and monitoring services, networking mode (VPC_NATIVE), and We determine the locations of the nodes (us-central1-b) in our first code block.

addons_config block,
Specifies the configuration of Kubernetes plugins.
The "http_load_balancing" and "horizontal_pod_autoscaling" blocks determine whether these plugins are enabled or disabled.
After determining the Kubernetes release channel to be used with "release_channel", we determine our kubernetes workload ID.


If "ip_allocation_policy" is in our code block,
We set the IP assignment policy in the Kubernetes cluster. Here,
For example, the "cluster_secondary_range_name" and "services_secondary_range_name" variables specify that the pod and services use a different IP range.
"enable_private_nodes" and "enable_private_endpoint" variables,
determines whether custom nodes and custom endpoints are enabled or disabled.
The variable "master_ipv4_cidr_block" specifies an IPV4 CIDR block for Kubernetes masters.


Step 8 Node Pools

Although I know that I can use the default service at this stage in Case, I still created a special service account to follow the apps.
I specified how many nodes I wanted and after defining the cluster and account id, I determined the machine type.
Then I defined IAM Role to my service account in my project.
I set it to true because I want the management configuration to stay the same and defined the minimum and maximum number of nodes in scaling


____________________________________________________________


To run Terraform on my computer, after I have authorized my application credentials, namely Gcloud auth.

Terraform init
Terraform apply
It will launch the Terraform backend to use bucket downloading the Google provider. We run terraform Apply to actually create all these resources I defined in terraform.

We take Docker Build and tag the app in our local with docker tag. After pushing Docker
We change the image as kubectl edit deployment/podname on GCK Shell

kubectl apply -f {my_yaml_path}

kubectl get ns
kubectl get pods -n staging
kubectl exec -n staging -it gcloud-548d547b84-zdnv6 -- bash
	gcloud alpha storage ls 
kubectl get sa -n staging

//helm repo add ingress nginx https://kubernetes.github.io/ingress-nginx
//helm search repo nginx




